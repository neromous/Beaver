#!/usr/bin/env python3
"""
Beaver æµå¼æ¨ç†åŠŸèƒ½æ¼”ç¤º

å±•ç¤ºå¦‚ä½•ä½¿ç”¨æµå¼OpenAI APIæ¨ç†åŠŸèƒ½ï¼Œå®ç°å®æ—¶æ–‡æœ¬ç”Ÿæˆ
"""

import sys
import os
import time
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def demo_basic_streaming():
    """åŸºç¡€æµå¼èŠå¤©æ¼”ç¤º"""
    print("ğŸŒŠ åŸºç¡€æµå¼èŠå¤©æ¼”ç¤º")
    print("-" * 40)
    
    try:
        from beaver.config import config_manager
        from beaver.inference import stream_simple_chat, collect_stream_response
        
        # åŠ è½½é…ç½®
        config = config_manager.load_config('resources/config.json')
        default_config = config['default']
        
        api_url = default_config['api/url']
        api_key = default_config['api/sk']
        model = default_config['api/model']
        
        print(f"ä½¿ç”¨æ¨¡å‹: {model}")
        
        # ç¤ºä¾‹é—®é¢˜
        questions = [
            "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½",
            "å†™ä¸€ä¸ªç®€çŸ­çš„Pythonå‡½æ•°ç¤ºä¾‹",
            "æ¨èä¸€æœ¬å¥½ä¹¦å¹¶è¯´æ˜ç†ç”±"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ’¬ é—®é¢˜ {i}: {question}")
            print("ğŸ¤– AIå›å¤: ", end="", flush=True)
            
            # æµå¼è¾“å‡ºï¼Œå®æ—¶æ˜¾ç¤º
            full_response = ""
            for chunk in stream_simple_chat(
                prompt=question,
                api_url=api_url,
                api_key=api_key,
                model=model,
                max_tokens=150,
                timeout=30
            ):
                print(chunk, end="", flush=True)
                full_response += chunk
                time.sleep(0.02)  # æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœ
            
            print()  # æ¢è¡Œ
            print(f"ğŸ“Š å®Œæ•´å›å¤é•¿åº¦: {len(full_response)} å­—ç¬¦")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºç¡€æµå¼æ¼”ç¤ºå¤±è´¥: {e}")
        return False

def demo_streaming_with_callback():
    """å¸¦å›è°ƒå‡½æ•°çš„æµå¼æ¼”ç¤º"""
    print("\nğŸ“ å¸¦å›è°ƒå‡½æ•°çš„æµå¼æ¼”ç¤º")
    print("-" * 40)
    
    try:
        from beaver.config import config_manager
        from beaver.inference import stream_simple_chat
        
        # åŠ è½½é…ç½®
        config = config_manager.load_config('resources/config.json')
        default_config = config['default']
        
        api_url = default_config['api/url']
        api_key = default_config['api/sk']
        model = default_config['api/model']
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'chunks': 0,
            'total_chars': 0,
            'words': 0
        }
        
        # å›è°ƒå‡½æ•°
        def on_chunk_received(chunk):
            stats['chunks'] += 1
            stats['total_chars'] += len(chunk)
            # ç®€å•è®¡ç®—å•è¯æ•°
            if chunk.strip():
                stats['words'] += len(chunk.split())
            
            # å®æ—¶æ˜¾ç¤ºç»Ÿè®¡
            print(f"\rğŸ“Š å·²æ¥æ”¶: {stats['chunks']} å— | {stats['total_chars']} å­—ç¬¦ | {stats['words']} è¯", 
                  end="", flush=True)
        
        question = "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"
        print(f"ğŸ’¬ é—®é¢˜: {question}")
        print("ğŸ¤– AIå›å¤:")
        
        # ä½¿ç”¨å›è°ƒçš„æµå¼èŠå¤©
        full_response = ""
        for chunk in stream_simple_chat(
            prompt=question,
            api_url=api_url,
            api_key=api_key,
            model=model,
            max_tokens=200,
            timeout=30,
            on_chunk=on_chunk_received
        ):
            full_response += chunk
        
        print(f"\nğŸ“ å®Œæ•´è¯—æ­Œ:\n{full_response}")
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡: {stats['chunks']} å—, {stats['total_chars']} å­—ç¬¦, {stats['words']} è¯")
        
        return True
        
    except Exception as e:
        print(f"âŒ å›è°ƒæ¼”ç¤ºå¤±è´¥: {e}")
        return False

def demo_streaming_conversation():
    """æµå¼å¤šè½®å¯¹è¯æ¼”ç¤º"""
    print("\nğŸ’¬ æµå¼å¤šè½®å¯¹è¯æ¼”ç¤º")
    print("-" * 40)
    
    try:
        from beaver.config import config_manager
        from beaver.inference import stream_chat_completion
        
        # åŠ è½½é…ç½®
        config = config_manager.load_config('resources/config.json')
        default_config = config['default']
        
        api_url = default_config['api/url']
        api_key = default_config['api/sk']
        model = default_config['api/model']
        
        # æ„å»ºå¤šè½®å¯¹è¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ç¼–ç¨‹åŠ©æ‰‹"},
            {"role": "user", "content": "æˆ‘æƒ³å­¦ä¹ Python"},
            {"role": "assistant", "content": "å¤ªå¥½äº†ï¼Pythonæ˜¯ä¸€é—¨å¾ˆæ£’çš„ç¼–ç¨‹è¯­è¨€ã€‚ä½ æƒ³ä»å“ªé‡Œå¼€å§‹å‘¢ï¼Ÿ"},
            {"role": "user", "content": "è¯·æ¨èä¸€äº›å­¦ä¹ èµ„æºå’Œè·¯å¾„"}
        ]
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        print("ğŸ“œ å¯¹è¯å†å²:")
        for msg in messages:
            role_icons = {"system": "âš™ï¸", "user": "ğŸ‘¤", "assistant": "ğŸ¤–"}
            icon = role_icons.get(msg['role'], 'ğŸ’¬')
            print(f"{icon} {msg['role']}: {msg['content']}")
        
        print("\nğŸ¤– AIç»§ç»­å›å¤: ", end="", flush=True)
        
        # æµå¼å¤šè½®å¯¹è¯
        full_response = ""
        for chunk in stream_chat_completion(
            messages=messages,
            api_url=api_url,
            api_key=api_key,
            model=model,
            temperature=0.7,
            max_tokens=300,
            timeout=30
        ):
            # æå–æ–‡æœ¬å†…å®¹
            if 'choices' in chunk and chunk['choices']:
                choice = chunk['choices'][0]
                if 'delta' in choice and 'content' in choice['delta']:
                    content = choice['delta']['content']
                    if content:
                        print(content, end="", flush=True)
                        full_response += content
                        time.sleep(0.01)
        
        print(f"\n\nğŸ“ å®Œæ•´å›å¤:\n{full_response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤šè½®å¯¹è¯æ¼”ç¤ºå¤±è´¥: {e}")
        return False

def demo_streaming_with_progress():
    """å¸¦è¿›åº¦æ˜¾ç¤ºçš„æµå¼æ¼”ç¤º"""
    print("\nğŸ“Š å¸¦è¿›åº¦æ˜¾ç¤ºçš„æµå¼æ¼”ç¤º")
    print("-" * 40)
    
    try:
        from beaver.config import config_manager
        from beaver.inference import stream_simple_chat, stream_with_progress
        
        # åŠ è½½é…ç½®
        config = config_manager.load_config('resources/config.json')
        default_config = config['default']
        
        api_url = default_config['api/url']
        api_key = default_config['api/sk']
        model = default_config['api/model']
        
        question = "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
        print(f"ğŸ’¬ é—®é¢˜: {question}")
        
        # åˆ›å»ºæµ
        stream = stream_simple_chat(
            prompt=question,
            api_url=api_url,
            api_key=api_key,
            model=model,
            max_tokens=200,
            timeout=30
        )
        
        # ä½¿ç”¨è¿›åº¦æ˜¾ç¤ºçš„æµå¤„ç†
        print("ğŸ”„ å¼€å§‹æµå¼å¤„ç†...")
        full_response = ""
        for chunk in stream_with_progress(stream, show_progress=True, chunk_delay=0.05):
            full_response += chunk
        
        print(f"\nğŸ“ å®Œæ•´å›å¤:\n{full_response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¿›åº¦æ¼”ç¤ºå¤±è´¥: {e}")
        return False

def demo_streaming_different_temperatures():
    """ä¸åŒæ¸©åº¦å‚æ•°çš„æµå¼æ¼”ç¤º"""
    print("\nğŸŒ¡ï¸ ä¸åŒæ¸©åº¦å‚æ•°çš„æµå¼æ¼”ç¤º")
    print("-" * 40)
    
    try:
        from beaver.config import config_manager
        from beaver.inference import stream_simple_chat, collect_stream_response
        
        # åŠ è½½é…ç½®
        config = config_manager.load_config('resources/config.json')
        default_config = config['default']
        
        api_url = default_config['api/url']
        api_key = default_config['api/sk']
        model = default_config['api/model']
        
        question = "ç”¨å‡ ä¸ªè¯æè¿°äººå·¥æ™ºèƒ½"
        temperatures = [0.1, 0.7, 1.5]
        
        print(f"ğŸ’¬ é—®é¢˜: {question}")
        
        for temp in temperatures:
            print(f"\nğŸŒ¡ï¸ æ¸©åº¦ {temp}:")
            print("ğŸ¤– å›å¤: ", end="", flush=True)
            
            # ä¸åŒæ¸©åº¦çš„æµå¼è¾“å‡º
            stream = stream_simple_chat(
                prompt=question,
                api_url=api_url,
                api_key=api_key,
                model=model,
                temperature=temp,
                max_tokens=50,
                timeout=20
            )
            
            response = collect_stream_response(stream)
            print(response)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¸©åº¦æ¼”ç¤ºå¤±è´¥: {e}")
        return False

def demo_interactive_streaming():
    """äº¤äº’å¼æµå¼èŠå¤©æ¼”ç¤º"""
    print("\nğŸ® äº¤äº’å¼æµå¼èŠå¤©æ¼”ç¤º")
    print("-" * 40)
    print("è¾“å…¥ 'quit' é€€å‡ºï¼Œè¾“å…¥ 'clear' æ¸…ç©ºå†å²")
    
    try:
        from beaver.config import config_manager
        from beaver.inference import stream_chat_completion
        
        # åŠ è½½é…ç½®
        config = config_manager.load_config('resources/config.json')
        default_config = config['default']
        
        api_url = default_config['api/url']
        api_key = default_config['api/sk']
        model = default_config['api/model']
        
        # å¯¹è¯å†å²
        messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"}]
        
        while True:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            if user_input.lower() == 'quit':
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if user_input.lower() == 'clear':
                messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"}]
                print("ğŸ§¹ å†å²å·²æ¸…ç©º")
                continue
            
            if not user_input:
                continue
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": user_input})
            
            print("ğŸ¤– AI: ", end="", flush=True)
            
            # æµå¼å›å¤
            ai_response = ""
            for chunk in stream_chat_completion(
                messages=messages,
                api_url=api_url,
                api_key=api_key,
                model=model,
                temperature=0.7,
                max_tokens=200,
                timeout=30
            ):
                if 'choices' in chunk and chunk['choices']:
                    choice = chunk['choices'][0]
                    if 'delta' in choice and 'content' in choice['delta']:
                        content = choice['delta']['content']
                        if content:
                            print(content, end="", flush=True)
                            ai_response += content
            
            # æ·»åŠ AIå›å¤åˆ°å†å²
            if ai_response:
                messages.append({"role": "assistant", "content": ai_response})
            
            print()  # æ¢è¡Œ
        
        return True
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
        return True
    except Exception as e:
        print(f"âŒ äº¤äº’æ¼”ç¤ºå¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµå¼æ¨ç†æ¼”ç¤º"""
    print("ğŸŒŠ Beaver æµå¼æ¨ç†åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    demos = [
        demo_basic_streaming,
        demo_streaming_with_callback, 
        demo_streaming_conversation,
        demo_streaming_with_progress,
        demo_streaming_different_temperatures
    ]
    
    passed = 0
    for demo in demos:
        if demo():
            passed += 1
    
    print(f"\nğŸ“Š æ¼”ç¤ºç»“æœ: {passed}/{len(demos)} ä¸ªæˆåŠŸ")
    
    if passed > 0:
        print("\nğŸ‰ æµå¼æ¨ç†æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ å¿«é€Ÿä½¿ç”¨æŒ‡å—:")
        print("```python")
        print("from beaver.inference import stream_simple_chat")
        print("")
        print("# å®æ—¶æµå¼è¾“å‡º")
        print("for chunk in stream_simple_chat('ä½ å¥½', api_url, api_key, model):")
        print("    print(chunk, end='', flush=True)")
        print("")
        print("# æ”¶é›†å®Œæ•´å“åº”")
        print("from beaver.inference import collect_stream_response")
        print("response = collect_stream_response(stream)")
        print("```")
        
        # è¯¢é—®æ˜¯å¦è¿›è¡Œäº¤äº’æ¼”ç¤º
        choice = input("\nğŸ® æ˜¯å¦è¿›è¡Œäº¤äº’å¼æ¼”ç¤º? (y/n): ").strip().lower()
        if choice == 'y':
            demo_interactive_streaming()
    else:
        print("âŒ æµå¼æ¨ç†æ¼”ç¤ºå¤±è´¥")

if __name__ == '__main__':
    main() 
#!/usr/bin/env python3
"""
Beaver Nexus Sync LLM Module

åŒæ­¥LLMæ¨ç†æ¨¡å—ï¼Œé›†æˆæ¶ˆæ¯è½¬æ¢å’Œé…ç½®ç®¡ç†
ç›®æ ‡ï¼š[":nexus/sync", {"provider": "", "model": ""}, [":msg/v2m", [":user", "foo"]]]

ä¸‰å±‚æ¶æ„ï¼š
1. åŸå§‹åŠŸèƒ½å‡½æ•°
2. Wrapper å‡½æ•°ï¼ˆæ–‡æœ¬åŒ–ç»“æœï¼‰
3. æ³¨å†Œå‡½æ•°ï¼ˆDSL å‘½ä»¤ï¼‰
"""

from typing import Dict, List, Any, Optional, Union
import json
import time


# ================================
# ç¬¬ä¸€å±‚ï¼šåŸå§‹åŠŸèƒ½å‡½æ•°
# ================================

def sync_llm_processor(
    config: Dict[str, str], 
    messages_expr: List[Any]
) -> Dict[str, Any]:
    """
    åŒæ­¥LLMæ¨ç†å¤„ç†å™¨
    
    å‚æ•°:
        config: é…ç½®å­—å…¸ï¼Œå¦‚ {"provider": "openai", "model": "gpt-4"}
        messages_expr: æ¶ˆæ¯è¡¨è¾¾å¼ï¼Œå¦‚ [":msg/v2m", [":user", "hello"]]
    
    è¿”å›:
        Dict[str, Any]: æ¨ç†ç»“æœï¼ŒåŒ…å«å›å¤å†…å®¹å’Œå…ƒæ•°æ®
    
    å¼‚å¸¸:
        ValueError: æ— æ•ˆçš„é…ç½®æˆ–æ¶ˆæ¯è¡¨è¾¾å¼
    """
    if not isinstance(config, dict):
        raise ValueError("é…ç½®å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
    
    if not isinstance(messages_expr, list):
        raise ValueError("æ¶ˆæ¯è¡¨è¾¾å¼å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼")
    
    # è½¬æ¢EDNå…³é”®å­—æ ¼å¼ä¸ºPythonå­—ç¬¦ä¸²
    normalized_config = {}
    for key, value in config.items():
        # å¤„ç†EDNå…³é”®å­—ï¼ˆå¦‚ ':provider' -> 'provider'ï¼‰
        if isinstance(key, str) and key.startswith(':'):
            normalized_key = key[1:]
        else:
            normalized_key = key
        normalized_config[normalized_key] = value
    
    # éªŒè¯é…ç½®
    required_fields = ["provider", "model"]
    for field in required_fields:
        if field not in normalized_config:
            raise ValueError(f"é…ç½®ä¸­ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        if not normalized_config[field]:
            raise ValueError(f"é…ç½®å­—æ®µ {field} ä¸èƒ½ä¸ºç©º")
    
    provider = normalized_config["provider"]
    model = normalized_config["model"]
    
    try:
        # å¯¼å…¥å¿…è¦æ¨¡å—
        from beaver.core.dispatcher import dispatch
        from beaver.inference.sync_client import chat_with_config
        
        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†æ¶ˆæ¯è¡¨è¾¾å¼ï¼Œè½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼
        messages_result = dispatch(messages_expr)
        
        # æ£€æŸ¥æ¶ˆæ¯è½¬æ¢ç»“æœ
        if isinstance(messages_result, str):
            # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºJSON
            try:
                messages_data = json.loads(messages_result)
            except json.JSONDecodeError:
                raise ValueError(f"æ¶ˆæ¯è½¬æ¢å¤±è´¥: {messages_result}")
        else:
            messages_data = messages_result
        
        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        if isinstance(messages_data, dict):
            # å•ä¸ªæ¶ˆæ¯ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
            messages_list = [messages_data]
        elif isinstance(messages_data, list):
            # æ¶ˆæ¯åˆ—è¡¨
            messages_list = messages_data
        else:
            raise ValueError(f"æ— æ•ˆçš„æ¶ˆæ¯æ ¼å¼: {type(messages_data)}")
        
        # éªŒè¯æ¶ˆæ¯å†…å®¹
        for i, msg in enumerate(messages_list):
            if not isinstance(msg, dict):
                raise ValueError(f"ç¬¬{i+1}ä¸ªæ¶ˆæ¯ä¸æ˜¯å­—å…¸æ ¼å¼: {msg}")
            if "role" not in msg or "content" not in msg:
                raise ValueError(f"ç¬¬{i+1}ä¸ªæ¶ˆæ¯ç¼ºå°‘roleæˆ–contentå­—æ®µ: {msg}")
        
        # ç¬¬äºŒæ­¥ï¼šè°ƒç”¨LLM API
        start_time = time.time()
        
        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºä¸»è¦æç¤ºè¯
        user_messages = [msg for msg in messages_list if msg["role"] == "user"]
        system_messages = [msg for msg in messages_list if msg["role"] == "system"]
        
        if not user_messages:
            raise ValueError("æ¶ˆæ¯åˆ—è¡¨ä¸­å¿…é¡»åŒ…å«è‡³å°‘ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯")
        
        # ä½¿ç”¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºæç¤ºè¯
        prompt = user_messages[-1]["content"]
        
        # ä½¿ç”¨ç¬¬ä¸€æ¡ç³»ç»Ÿæ¶ˆæ¯ä½œä¸ºç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        system_prompt = system_messages[0]["content"] if system_messages else None
        
        # è°ƒç”¨LLM
        response_text = chat_with_config(
            prompt=prompt,
            provider=provider,
            model=model,
            system_prompt=system_prompt
        )
        
        end_time = time.time()
        response_time = round(end_time - start_time, 2)
        
        # æ„å»ºç»“æœ
        result = {
            "success": True,
            "response": response_text,
            "config": {
                "provider": provider,
                "model": model
            },
            "messages": messages_list,
            "metadata": {
                "response_time": response_time,
                "message_count": len(messages_list),
                "user_message_count": len(user_messages),
                "system_message_count": len(system_messages)
            }
        }
        
        return result
        
    except Exception as e:
        # è¿”å›é”™è¯¯ç»“æœ
        return {
            "success": False,
            "error": str(e),
            "config": {
                "provider": provider,
                "model": model
            },
            "messages": None,
            "metadata": {
                "response_time": 0,
                "message_count": 0
            }
        }


def batch_sync_llm_processor(
    configs_and_messages: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡åŒæ­¥LLMæ¨ç†å¤„ç†å™¨
    
    å‚æ•°:
        configs_and_messages: é…ç½®å’Œæ¶ˆæ¯è¡¨è¾¾å¼åˆ—è¡¨
    
    è¿”å›:
        List[Dict[str, Any]]: æ‰¹é‡æ¨ç†ç»“æœåˆ—è¡¨
    """
    if not isinstance(configs_and_messages, list):
        raise ValueError("è¾“å…¥å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼")
    
    results = []
    for i, item in enumerate(configs_and_messages):
        try:
            if not isinstance(item, dict):
                raise ValueError(f"ç¬¬{i+1}é¡¹ä¸æ˜¯å­—å…¸æ ¼å¼")
            
            if "config" not in item or "messages" not in item:
                raise ValueError(f"ç¬¬{i+1}é¡¹ç¼ºå°‘configæˆ–messageså­—æ®µ")
            
            result = sync_llm_processor(item["config"], item["messages"])
            results.append(result)
            
        except Exception as e:
            # æ·»åŠ é”™è¯¯ç»“æœ
            results.append({
                "success": False,
                "error": f"å¤„ç†ç¬¬{i+1}é¡¹æ—¶å‡ºé”™: {str(e)}",
                "config": item.get("config", {}),
                "messages": None,
                "metadata": {"response_time": 0, "message_count": 0}
            })
    
    return results


def sync_llm_validator(config: Dict[str, str]) -> Dict[str, Any]:
    """
    éªŒè¯åŒæ­¥LLMé…ç½®æ˜¯å¦æœ‰æ•ˆ
    
    å‚æ•°:
        config: é…ç½®å­—å…¸
    
    è¿”å›:
        Dict[str, Any]: éªŒè¯ç»“æœ
    """
    try:
        from beaver.inference.sync_client import validate_openai_config
        from beaver.config.helpers import get_api_config
        
        if not isinstance(config, dict):
            return {"valid": False, "error": "é…ç½®å¿…é¡»æ˜¯å­—å…¸æ ¼å¼"}
        
        # è½¬æ¢EDNå…³é”®å­—æ ¼å¼ä¸ºPythonå­—ç¬¦ä¸²
        normalized_config = {}
        for key, value in config.items():
            # å¤„ç†EDNå…³é”®å­—ï¼ˆå¦‚ ':provider' -> 'provider'ï¼‰
            if isinstance(key, str) and key.startswith(':'):
                normalized_key = key[1:]
            else:
                normalized_key = key
            normalized_config[normalized_key] = value
        
        provider = normalized_config.get("provider")
        model = normalized_config.get("model")
        
        if not provider or not model:
            return {"valid": False, "error": "é…ç½®ä¸­ç¼ºå°‘provideræˆ–model"}
        
        # è·å–APIé…ç½®
        api_config = get_api_config(provider, model)
        if not api_config:
            return {"valid": False, "error": f"æœªæ‰¾åˆ° {provider}/{model} çš„é…ç½®"}
        
        # éªŒè¯APIè¿æ¥
        validation_result = validate_openai_config(
            api_url=api_config.get('url'),
            api_key=api_config.get('secret_key'),
            model=api_config.get('model')
        )
        
        return validation_result
        
    except Exception as e:
        return {"valid": False, "error": f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}"}


# ================================
# ç¬¬äºŒå±‚ï¼šWrapper å‡½æ•°ï¼ˆæ–‡æœ¬åŒ–ç»“æœï¼‰
# ================================

def sync_llm_wrapper(config: Dict[str, str], messages_expr: List[Any]) -> str:
    """
    åŒæ­¥LLMæ¨ç†çš„wrapperå‡½æ•°ï¼Œè¿”å›æ–‡æœ¬åŒ–ç»“æœ
    
    å‚æ•°:
        config: é…ç½®å­—å…¸
        messages_expr: æ¶ˆæ¯è¡¨è¾¾å¼
    
    è¿”å›:
        str: æ¨ç†ç»“æœçš„æ–‡æœ¬æè¿°
    """
    try:
        result = sync_llm_processor(config, messages_expr)
        
        if result["success"]:
            provider = result["config"]["provider"]
            model = result["config"]["model"]
            response_time = result["metadata"]["response_time"]
            message_count = result["metadata"]["message_count"]
            
            output_lines = [
                f"ğŸ¤– LLMæ¨ç†æˆåŠŸ [{provider}/{model}]",
                f"â±ï¸ å“åº”æ—¶é—´: {response_time}ç§’",
                f"ğŸ“¨ å¤„ç†æ¶ˆæ¯æ•°: {message_count}",
                "",
                "ğŸ’¬ AIå›å¤:",
                result["response"]
            ]
            
            return "\n".join(output_lines)
        else:
            provider = result["config"]["provider"]
            model = result["config"]["model"]
            
            return f"âŒ LLMæ¨ç†å¤±è´¥ [{provider}/{model}]: {result['error']}"
            
    except Exception as e:
        return f"âŒ æ¨ç†è¿‡ç¨‹å¼‚å¸¸: {str(e)}"


def batch_sync_llm_wrapper(configs_and_messages: List[Dict[str, Any]]) -> str:
    """
    æ‰¹é‡åŒæ­¥LLMæ¨ç†çš„wrapperå‡½æ•°
    
    å‚æ•°:
        configs_and_messages: é…ç½®å’Œæ¶ˆæ¯è¡¨è¾¾å¼åˆ—è¡¨
    
    è¿”å›:
        str: æ‰¹é‡æ¨ç†ç»“æœçš„æ–‡æœ¬æè¿°
    """
    try:
        results = batch_sync_llm_processor(configs_and_messages)
        
        output_lines = [f"ğŸ”„ æ‰¹é‡LLMæ¨ç†å®Œæˆ - å¤„ç† {len(results)} é¡¹:"]
        
        success_count = 0
        for i, result in enumerate(results, 1):
            if result["success"]:
                success_count += 1
                provider = result["config"]["provider"]
                model = result["config"]["model"]
                response_time = result["metadata"]["response_time"]
                
                output_lines.extend([
                    f"",
                    f"{i}. âœ… [{provider}/{model}] ({response_time}s)",
                    f"   {result['response'][:100]}{'...' if len(result['response']) > 100 else ''}"
                ])
            else:
                provider = result["config"].get("provider", "unknown")
                model = result["config"].get("model", "unknown")
                
                output_lines.extend([
                    f"",
                    f"{i}. âŒ [{provider}/{model}]",
                    f"   {result['error']}"
                ])
        
        output_lines.insert(1, f"ğŸ“Š æˆåŠŸç‡: {success_count}/{len(results)}")
        
        return "\n".join(output_lines)
        
    except Exception as e:
        return f"âŒ æ‰¹é‡æ¨ç†å¼‚å¸¸: {str(e)}"


def sync_llm_validator_wrapper(config: Dict[str, str]) -> str:
    """
    åŒæ­¥LLMé…ç½®éªŒè¯çš„wrapperå‡½æ•°
    
    å‚æ•°:
        config: é…ç½®å­—å…¸
    
    è¿”å›:
        str: éªŒè¯ç»“æœçš„æ–‡æœ¬æè¿°
    """
    result = sync_llm_validator(config)
    
    if result["valid"]:
        response_time = result.get("response_time", 0)
        return f"âœ… é…ç½®éªŒè¯æˆåŠŸ ({response_time}ç§’)"
    else:
        error = result.get("error", "æœªçŸ¥é”™è¯¯")
        return f"âŒ é…ç½®éªŒè¯å¤±è´¥: {error}"


# ================================
# ç¬¬ä¸‰å±‚ï¼šæ³¨å†Œå‡½æ•°ï¼ˆDSL å‘½ä»¤ï¼‰
# ================================

from beaver.core.decorators import bf_element

@bf_element(
    ':nexus/sync',
    description='åŒæ­¥LLMæ¨ç†ï¼Œæ•´åˆé…ç½®å’Œæ¶ˆæ¯è½¬æ¢',
    category='Nexus',
    usage="[':nexus/sync', {'provider': 'openai', 'model': 'gpt-4'}, [':msg/v2m', [':user', 'hello']]]")
def sync_llm_command(config: Dict[str, str], messages_expr: List[Any]):
    """
    åŒæ­¥LLMæ¨ç†å‘½ä»¤
    
    ç”¨æ³•: [:nexus/sync {"provider": "openai", "model": "gpt-4"} [":msg/v2m" [":user" "hello"]]]
    """
    if not isinstance(config, dict):
        return "âŒ ç¬¬ä¸€ä¸ªå‚æ•°å¿…é¡»æ˜¯é…ç½®å­—å…¸"
    
    if not isinstance(messages_expr, list):
        return "âŒ ç¬¬äºŒä¸ªå‚æ•°å¿…é¡»æ˜¯æ¶ˆæ¯è¡¨è¾¾å¼åˆ—è¡¨"
    
    return sync_llm_wrapper(config, messages_expr)

@bf_element(
    ':nexus/sync-batch',
    description='æ‰¹é‡åŒæ­¥LLMæ¨ç†',
    category='Nexus',
    usage="[':nexus/sync-batch', [{'config': {...}, 'messages': [...]}]]")
def batch_sync_llm_command(configs_and_messages: List[Dict[str, Any]]):
    """
    æ‰¹é‡åŒæ­¥LLMæ¨ç†å‘½ä»¤
    
    ç”¨æ³•: [:nexus/sync-batch [{"config": {...}, "messages": [...]}]]
    """
    if not isinstance(configs_and_messages, list):
        return "âŒ å‚æ•°å¿…é¡»æ˜¯é…ç½®å’Œæ¶ˆæ¯è¡¨è¾¾å¼åˆ—è¡¨"
    
    return batch_sync_llm_wrapper(configs_and_messages)

@bf_element(
    ':nexus/validate',
    description='éªŒè¯LLMé…ç½®æ˜¯å¦æœ‰æ•ˆ',
    category='Nexus',
    usage="[':nexus/validate', {'provider': 'openai', 'model': 'gpt-4'}]")
def sync_llm_validate_command(config: Dict[str, str]):
    """
    LLMé…ç½®éªŒè¯å‘½ä»¤
    
    ç”¨æ³•: [:nexus/validate {"provider": "openai", "model": "gpt-4"}]
    """
    if not isinstance(config, dict):
        return "âŒ å‚æ•°å¿…é¡»æ˜¯é…ç½®å­—å…¸"
    
    return sync_llm_validator_wrapper(config)

@bf_element(
    ':nexus/quick-chat',
    description='å¿«é€ŸèŠå¤©ï¼Œä½¿ç”¨é»˜è®¤é…ç½®',
    category='Nexus',
    usage="[':nexus/quick-chat', 'user prompt']")
def quick_chat_command(prompt: str):
    """
    å¿«é€ŸèŠå¤©å‘½ä»¤ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    
    ç”¨æ³•: [:nexus/quick-chat "hello"]
    """
    if not isinstance(prompt, str):
        return "âŒ å‚æ•°å¿…é¡»æ˜¯å­—ç¬¦ä¸²"
    
    try:
        from beaver.config.helpers import get_default_provider
        
        # è·å–é»˜è®¤é…ç½®
        default_config = get_default_provider()
        if not default_config:
            return "âŒ æœªæ‰¾åˆ°é»˜è®¤é…ç½®ï¼Œè¯·å…ˆè®¾ç½®é»˜è®¤çš„providerå’Œmodel"
        
        provider = default_config.get("provider")
        model = default_config.get("model")
        
        if not provider or not model:
            return "âŒ é»˜è®¤é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘provideræˆ–model"
        
        config = {"provider": provider, "model": model}
        messages_expr = [":msg/v2m", [":user", prompt]]
        
        return sync_llm_wrapper(config, messages_expr)
        
    except Exception as e:
        return f"âŒ å¿«é€ŸèŠå¤©å¤±è´¥: {str(e)}" 